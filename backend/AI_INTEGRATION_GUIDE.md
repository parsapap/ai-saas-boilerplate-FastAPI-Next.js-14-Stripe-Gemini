# ğŸ¤– AI Integration Guide - Gemini & Multi-Model Support

## ğŸ“‹ Overview

Ø³ÛŒØ³ØªÙ… AI Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ:
- âœ… 4 Ù…Ø¯Ù„ AI: Gemini 1.5 Flash/Pro, Claude 3 Haiku, GPT-4o Mini
- âœ… Rate limiting Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Redis
- âœ… Usage tracking Ø¯Ù‚ÛŒÙ‚ (messages + tokens)
- âœ… Plan-based limits (Free/Pro/Team)
- âœ… Streaming support (SSE)
- âœ… Background tasks Ø¨Ø§ Celery
- âœ… Cost tracking

---

## ğŸ¯ AI Models

### Gemini 1.5 Flash (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ)
```
Provider: Google
Speed: âš¡âš¡âš¡ Ø®ÛŒÙ„ÛŒ Ø³Ø±ÛŒØ¹
Cost: $ (Ø§Ø±Ø²Ø§Ù†)
Max Tokens: 8,192
Best for: Chat, Q&A, Ø³Ø±ÛŒØ¹
```

### Gemini 1.5 Pro
```
Provider: Google
Speed: âš¡âš¡ Ù…ØªÙˆØ³Ø·
Cost: $$ (Ù…ØªÙˆØ³Ø·)
Max Tokens: 8,192
Best for: ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ØŒ reasoning
```

### Claude 3 Haiku
```
Provider: Anthropic
Speed: âš¡âš¡âš¡ Ø³Ø±ÛŒØ¹
Cost: $ (Ø§Ø±Ø²Ø§Ù†)
Max Tokens: 4,096
Best for: ChatØŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
```

### GPT-4o Mini
```
Provider: OpenAI
Speed: âš¡âš¡ Ù…ØªÙˆØ³Ø·
Cost: $ (Ø§Ø±Ø²Ø§Ù†)
Max Tokens: 16,384
Best for: Ù‡Ù…Ù‡â€ŒÚ©Ø§Ø±Ù‡
```

---

## ğŸ“Š Plan-Based Limits

| Feature | Free | Pro | Team |
|---------|------|-----|------|
| **Messages/Month** | 50 | 10,000 | âˆ Unlimited |
| **Tokens/Month** | 50K | 10M | âˆ Unlimited |
| **Rate Limit** | 5/min | 60/min | 300/min |
| **Max Tokens/Request** | 1,024 | 4,096 | 8,192 |
| **Models** | Flash, GPT-4o Mini | Ù‡Ù…Ù‡ | Ù‡Ù…Ù‡ |

---

## ğŸš€ API Endpoints

### 1. Chat Completion

```bash
POST /api/v1/ai/chat
```

**Request:**
```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "Ø³Ù„Ø§Ù…! Ú†Ø·ÙˆØ±ÛŒØŸ"}
  ],
  "model": "gemini-1.5-flash",
  "temperature": 0.7,
  "max_tokens": 1024,
  "stream": false
}
```

**Response:**
```json
{
  "message": "Ø³Ù„Ø§Ù…! Ù…Ù† Ø®ÙˆØ¨Ù…ØŒ Ù…Ù…Ù†ÙˆÙ†! Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ú©Ù…Ú©Øª Ú©Ù†Ù…ØŸ",
  "model": "gemini-1.5-flash",
  "usage": {
    "input_tokens": 15,
    "output_tokens": 20,
    "total_tokens": 35
  },
  "finish_reason": "stop"
}
```

---

### 2. Streaming Chat

```bash
POST /api/v1/ai/chat/stream
```

**Response (SSE):**
```
data: Ø³Ù„Ø§Ù…

data: !

data:  Ù…Ù†

data:  Ø®ÙˆØ¨Ù…

data: [DONE]
```

**JavaScript Example:**
```javascript
const eventSource = new EventSource('/api/v1/ai/chat/stream');

eventSource.onmessage = (event) => {
  if (event.data === '[DONE]') {
    eventSource.close();
    return;
  }
  console.log(event.data);
};
```

---

### 3. Get Usage Stats

```bash
GET /api/v1/ai/usage
```

**Response:**
```json
{
  "total_messages": 45,
  "total_tokens": 12500,
  "messages_limit": 50,
  "tokens_limit": 50000,
  "usage_percentage": 90.0,
  "models_breakdown": {
    "gemini-1.5-flash": {
      "messages": 40,
      "tokens": 11000,
      "cost": 15
    },
    "gpt-4o-mini": {
      "messages": 5,
      "tokens": 1500,
      "cost": 3
    }
  }
}
```

---

### 4. Get Available Models

```bash
GET /api/v1/ai/models
```

**Response:**
```json
{
  "plan": "pro",
  "models": [
    {
      "id": "gemini-1.5-flash",
      "name": "Gemini 1.5 Flash",
      "provider": "google",
      "max_tokens": 8192
    },
    {
      "id": "gemini-1.5-pro",
      "name": "Gemini 1.5 Pro",
      "provider": "google",
      "max_tokens": 8192
    }
  ]
}
```

---

## ğŸ”§ Setup

### 1. Get API Keys

#### Gemini (Google)
```
1. Ø¨Ø±Ùˆ Ø¨Ù‡: https://makersuite.google.com/app/apikey
2. Create API Key
3. Ú©Ù¾ÛŒ Ú©Ù†
```

#### OpenAI
```
1. Ø¨Ø±Ùˆ Ø¨Ù‡: https://platform.openai.com/api-keys
2. Create new secret key
3. Ú©Ù¾ÛŒ Ú©Ù†
```

#### Anthropic (Claude)
```
1. Ø¨Ø±Ùˆ Ø¨Ù‡: https://console.anthropic.com/
2. Get API Key
3. Ú©Ù¾ÛŒ Ú©Ù†
```

---

### 2. Environment Variables

```env
# backend/.env
GEMINI_API_KEY=AIzaSy...
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
```

---

### 3. Run Migrations

```bash
docker-compose exec backend alembic upgrade head
```

---

### 4. Start Celery Worker (Optional)

```bash
# For background tasks
docker-compose exec backend celery -A app.tasks.celery_app worker --loglevel=info
```

---

## ğŸ§ª Testing

### Test Script

```bash
cd backend
./test_ai.sh
```

### Manual Test

```bash
# 1. Login
TOKEN=$(curl -s -X POST http://localhost:8000/api/v1/auth/login \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=user@test.com&password=pass123" | jq -r '.access_token')

# 2. Create org
ORG_ID=$(curl -s -X POST http://localhost:8000/api/v1/orgs \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"name":"Test","slug":"test"}' | jq -r '.id')

# 3. Chat
curl -X POST http://localhost:8000/api/v1/ai/chat \
  -H "Authorization: Bearer $TOKEN" \
  -H "X-Current-Org: $ORG_ID" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Ø³Ù„Ø§Ù…! Ú†Ø·ÙˆØ±ÛŒØŸ"}
    ],
    "model": "gemini-1.5-flash"
  }' | jq
```

---

## ğŸ’¡ Use Cases

### 1. Chatbot

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "What is FastAPI?"}
]

response = await ai_service.chat_completion(
    messages=messages,
    model="gemini-1.5-flash"
)
```

### 2. Content Generation

```python
messages = [
    {"role": "user", "content": "Write a blog post about AI"}
]

response = await ai_service.chat_completion(
    messages=messages,
    model="gemini-1.5-pro",
    max_tokens=2048
)
```

### 3. Code Assistant

```python
messages = [
    {"role": "user", "content": "Explain this Python code: def fib(n): ..."}
]

response = await ai_service.chat_completion(
    messages=messages,
    model="gpt-4o-mini"
)
```

### 4. Translation

```python
messages = [
    {"role": "user", "content": "Translate to Persian: Hello World"}
]

response = await ai_service.chat_completion(
    messages=messages,
    model="gemini-1.5-flash"
)
```

---

## ğŸ›¡ï¸ Rate Limiting

### How it Works

```
1. Per-Minute Rate Limit (Redis)
   - Free: 5 requests/min
   - Pro: 60 requests/min
   - Team: 300 requests/min

2. Monthly Limits (Redis + DB)
   - Messages count
   - Tokens count
   
3. Checks before each request:
   âœ“ Rate limit not exceeded?
   âœ“ Monthly limit not exceeded?
   âœ“ Model allowed in plan?
   âœ“ Max tokens within limit?
```

### Error Responses

```json
// Rate limit exceeded
{
  "detail": "Rate limit exceeded. Max 5 requests per minute."
}
// Status: 429

// Monthly limit exceeded
{
  "detail": "Monthly limit exceeded. Upgrade your plan to continue."
}
// Status: 402

// Model not allowed
{
  "detail": "Model gemini-1.5-pro not available in your plan. Upgrade to access."
}
// Status: 403
```

---

## ğŸ“ˆ Usage Tracking

### Database Tables

#### ai_usage (Daily aggregates)
```sql
- organization_id
- date
- model
- message_count
- input_tokens
- output_tokens
- total_tokens
- estimated_cost (cents)
```

#### ai_requests (Individual requests)
```sql
- organization_id
- user_id
- model
- input_tokens
- output_tokens
- duration_ms
- status (success/error)
- error_message
```

### Redis Keys

```
ai_usage:{org_id}:{year-month}:messages â†’ count
ai_usage:{org_id}:{year-month}:tokens â†’ count
ai_rate:{org_id}:minute â†’ rate limit counter
```

---

## âš¡ Background Tasks (Celery)

### Long-running Requests

```python
from app.tasks.ai_tasks import long_chat_completion

# Queue task
task = long_chat_completion.delay(
    messages=[{"role": "user", "content": "..."}],
    model="gemini-1.5-pro",
    org_id=1,
    user_id=1
)

# Check status
result = task.get(timeout=60)
```

### Batch Processing

```python
from app.tasks.ai_tasks import batch_process_messages

messages_batch = [
    {"id": 1, "messages": [...]},
    {"id": 2, "messages": [...]},
]

task = batch_process_messages.delay(
    messages_batch=messages_batch,
    model="gemini-1.5-flash",
    org_id=1
)
```

---

## ğŸ’° Cost Tracking

### Pricing (per 1K tokens)

| Model | Input | Output |
|-------|-------|--------|
| Gemini Flash | $0.00035 | $0.00105 |
| Gemini Pro | $0.00125 | $0.00375 |
| Claude Haiku | $0.00025 | $0.00125 |
| GPT-4o Mini | $0.00015 | $0.00060 |

### Example Calculation

```
Request:
- Input: 100 tokens
- Output: 200 tokens
- Model: Gemini Flash

Cost:
- Input: (100/1000) * $0.00035 = $0.000035
- Output: (200/1000) * $0.00105 = $0.00021
- Total: $0.000245 = 0.0245 cents

Stored as: 0 cents (rounded)
```

---

## ğŸš¨ Error Handling

```python
try:
    response = await ai_service.chat_completion(...)
except HTTPException as e:
    if e.status_code == 429:
        # Rate limit exceeded
        print("Too many requests, wait a minute")
    elif e.status_code == 402:
        # Monthly limit exceeded
        print("Upgrade your plan")
    elif e.status_code == 403:
        # Model not allowed
        print("Model not available in your plan")
```

---

## ğŸ“š Files Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ ai_usage.py          # Usage models
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ ai.py                # AI schemas
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ ai_service.py        # AI service
â”‚   â”œâ”€â”€ crud/
â”‚   â”‚   â””â”€â”€ ai_usage.py          # Usage CRUD
â”‚   â”œâ”€â”€ api/v1/
â”‚   â”‚   â””â”€â”€ ai.py                # AI endpoints
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ ai_config.py         # Models & limits
â”‚   â”‚   â””â”€â”€ rate_limiter.py      # Rate limiting
â”‚   â””â”€â”€ tasks/
â”‚       â”œâ”€â”€ celery_app.py        # Celery config
â”‚       â””â”€â”€ ai_tasks.py          # Background tasks
â”‚
â””â”€â”€ alembic/versions/
    â””â”€â”€ 004_add_ai_usage.py      # Migration
```

---

Ø³ÙˆØ§Ù„ Ø¯Ø§Ø±ÛŒØŸ ğŸ˜Š
